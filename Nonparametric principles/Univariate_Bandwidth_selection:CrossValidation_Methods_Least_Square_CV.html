<html>
<head>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
  <style>
    .references {color:red;}
    code {
      background-color: powderblue;
      white-space: pre;
    }
    .mycode{
      margin: 20px 0px 100px 10px;
      background-color: powderblue;
      width: 70%;
    }
    body{
      margin-left: 10%;
      margin-right: 10%;
    }
  </style>
</head>

<body>
  
  <h3>Univariate Bandwidth Selection: Cross Validation: Least Square Cross Validation</h3>
  
  <p>We focus on data-driven methods for selecting h. Normally, the result of nonparametric kernel estimation is not insensitive to the choice of kernel function. Nevertheless, when it concerns bandwidth selection, it is not the case. When we have more data, we have more flexibility, thus less bias but higher variability. </p>

  <p>We start with least square cross-validation, which provides an optimal bandiwidth to all x in support of `f(x)`.</p>

  <p>We minimize `\int[\hat{f}(x)-f\left(x\right)]^{2}dx`, or equivalently 

  <center>`\int\hat{f}(x)^{2}dx-2\int\hat{f}\left(x\right)f(x)dx`</center>
  </p>

  <p>The second part `\int\hat{f}\left(x\right)f(x)dx` is `E_{X}[\hat{f}(x)]`, which we can estimate by `n^{-1}\sum_{i=1}^{n}\hat{f}_{-i}(X_{i})`, where `\hat{f}_{-i}(X_{i})=\frac{1}{(n-1)h}\sum_{j=1,j\neq i}^{n}k(\frac{X_{i}-X_{j}}{h})` is the leave-one-out kernel estimator of `f(X_{i})`. </p>

  <p>The first term `\int\hat{f}(x)^{2}dx` will be estimated by

<center>`\int\hat{f}(x)^{2}dx=\frac{1}{n^{2}h^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\int k\left(\frac{X_{i}-x}{h}\right)k\left(\frac{X_{j}-x}{h}\right)dx`</center>

which is equal to 

<center>`\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}\bar{k}(\frac{X_{i}-X_{j}}{h})`</center> . where `\bar{k}=\int k(u)k(v-u)du` is the 2-fold convolution kernel given v.
  </p>

  <p>Least Squares cross-validation chooses h to minimize 

<center>`CV_{f}(h)=\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}\bar{k}(\frac{X_{i}-X_{j}}{h})-\frac{2}{n(n-1)h}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}k(\frac{X_{i}-X_{j}}{h})` </center>
  </p>

  <p>We now denote the leading term of `CV_{f}(h)` as `CV_{f0}(h)`.</p> 

  <p>Mathematically, it can be shown that as the optimal `\hat{h}_{f0}` which minimizes `CV_{f0}(h)` is equivalent to the `\hat{h}_{IMSE}` which minimizes IMSE of the last part. And the `(\hat{h}-\hat{h}_{f0})/\hat{h}_{f0}\rightarrow0` in probability. That is to say, our least square CV solution converges to the solution as if we knew the true distribution of `f(x)`. </p>

  <div class = 'references'>
    References.Books:
    <br>
    [1]. Nonparametric econometrics
    <br>
  </div>
  
  <div class ='mycode'>
  <code>
    library(ggplot2)
    library(magrittr) ## to use pipe operator
    
    ### Create new environments
    p <- new.env() # environment for parameters
    d <- new.env() # environment for data
    f <- new.env() # environment for functions
    
    ### Number of observations.
    p$n <- 2000
    
    ## The true distribution of our x is normal, with mean = 3, std = 1
    d$x <- rnorm(n = p$n, mean = 3, sd = 1)
    
    ### Standard normal kernel
    f[['kn_stdnorm']] <- function(x) {
      value <- exp(-(x^2)/2)/sqrt(2*pi)
      return(value)
    } 
    
    f[['kn_stdnorm_wrapper']] <- function(h, xi, x) {
      value <- f$kn_stdnorm((x-xi)/h)
      return(value)
    } 
    
    
    f[['fi']] <- function(h, i, x) {
      
      if (i == 1) {
        xnew <- x[c(2:length(x))]
      } else if (i == length(x)) {
        xnew <- x[c(1:(length(x)-1))]
      } else {
        xnew <- x[c(c(1:(i-1)),c((i+1):(length(x))))]
      }
      sum1 <- 0
      for (xj in xnew) {
        
        sum1 <- sum1 + f$kn_stdnorm((xj-x[i])/h)
      }
      return(sum1/(h*(length(x)-1)))
      
    }
    
    ### Leave one out kernel estimator:
    
    f$festexp <- function(h, x) {
      sum1 <- 0
      for (i in 1:length(x)) {
        sum1 <- sum1 + f$fi(h, i, x)
      }
      return(sum1/(length(x)-1))
    }
    
    
    
    # The computation is so much faster if we calculate the integral analytically.
    
    f$festsq <- function(h,x) {
      sum1 <- 0
      for (i in 1:length(x)) {
        for (j in 1:length(x)) {
          xi <- x[i]
          xj <- x[j]
          
          kcov <- function(x) {exp(-x^2/4)/sqrt(4*pi)}
          sum1 <- sum1 + kcov((xi-xj)/h)
        }
        
      }
      return(sum1/(h*(p$n)^2))
    }
    
    ## Cross validation function
    f$LSCV <- function(h,x=d$x) {
      return(f$festsq(h,x=d$x)-2 *f$festexp(h,x=d$x))
    }
    
    p$h <- seq(from = 0,to = 1,length.out = 100)
    p$h[1] <- p$h[1] + 0.000001
    
    p$ggplot1 <- ggplot(data = data.frame(h = 0), mapping = aes(x = h))
    p$ggplot1 + stat_function(fun = f$IMSE, color = 'red' ) + 
      stat_function(fun = f$LSCV, args = list(x=d$x), color = 'blue' ) + xlim(0,1)
    
    ## Verify that the LSCV solution converges to the IMSE solution
    
    which(f$IMSE(h = p$h) == min(f$IMSE(h = p$h))) %>% p$h[.]
    
    which(f$LSCV(h = p$h) == min(f$LSCV(h = p$h))) %>% p$h[.]
    
  </code>
  </div>
</body>
  
  <div class = 'references'>
    <p>References.Books</p>
    <p>[1]. Nonparametric Econometrics</p>
  </div>

</html>
