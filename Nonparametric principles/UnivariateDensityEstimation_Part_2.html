<head>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
  <style>
    .references {color:red;}
    code {
      background-color: powderblue;
      white-space: pre;
    }
    .mycode{
      margin: 20px 0px 100px 10px;
      background-color: powderblue;
      width: 70%;
    }
  </style>
</head>



<body>

<div id = 'General Kernel Estimators'>
<h1>Kernel Estimators. (Ref. [1]) </h1>
<br>
  <p>In general, a kernel estimator can be written in the following form:</p>
  <br>
  <br>
  <center>
`\hat{f(x)}=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{h}\times k(\frac{x_{i}-x}{h})`</center>
  <br>
  <br>
  We could use a other choices for kernel function, one is the standard normal kernel:
  <br>
  <br>
  <center>`k(v)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}v^{2}}`</center>
  <br>
  <br>
  <p>An important theoretical result is that when as long as the following 3 conditions are satistified, the kernel in questionwill be a consistent estimator of `f(x)`.
    <br>
    <center>`(i)` `\int k(v)dv=1`</center>
    <br>
    <center>`(ii)` `k(v)=k(-v)`</center>
    <br>
    <center>`(iii)` `\int v^{2}k(v)dv=\kappa_{2}>0`</center>
  <p> By consistency, we mean that `\hat{f}(x)` converges to `f(x)` in probability. It has something to do with the bias.</p>
  <br>
  <br>
  <p> to prove consistency, we have to look at the mean square error (MSE), which defined as </p>
  <center>`[[MSE(\hat{f}(x)), \equiv, E\{[\hat{f}\left(x\right)-f\left(x\right)]^{2}\}], 
    [ , =, E[\{\hat{f}(x)\}^{2}]-\{E[\hat{f}(x)]\}^{2}+[E(\hat{f}(x))-f(x)]^{2}],
    [ , \equiv, Var(\hat{f}(x))+[bias(\hat{f}(x))]^{2}]]` </center>
 <br>
 <p>where by Taylor expansion, we have </p>
 <br>
 <center>`bias(\hat{f}(x))=\frac{h^{2}}{2}f^{(2)}(x)\int v^{2}k(v)dv+O(h^{3})`</center>
 <br>
 <p>(Notice that here, we assume that `f(x)` is 3-time differentiable, and we can weaken the condition to 2 times differentiable.)</p>
 <br>
 <p>Now the variance could be calculated as </p>
 <center>`var(\hat{f}(x))=\frac{1}{nh}\{(\int k^{2}(v)dv)\times f(x)+O(h)\}`</center>
 <p>So that</p>
 <center>`[[MSE(\hat{f}(x)), = , \frac{h^{4}}{4}[\int v^{2}k(v)dv\times f^{(2)}(x)]^{2}+\frac{\int k^{2}(v)dv}{nh}+o(h^{4}+(nh)^{-1})],
   [,,O(h^{4}+(nh)^{-1})]]`</center>
 <p>which leads to </p>
 <center>`\hat{f}(x)-f(x)=O_{p}(h^{2}+(nh)^{-1/2})=o_{p}(1)`</center>
 <br>
 <p>Now two very important remarks:</p>
 <br>
 <p>1. by choosing `h=cn^{-1/\alpha}` with `\alpha > 1`, it is ensured that we can have a consistent estimator when `n\rightarrow\infty`</p>
 <br>
 <p>2. there is an optiomal `h_{opt}=c(x)\times n^{-1/5}`, where `c(x)=\{(\int k^{2}(v)dv)\times f(x)/[\int v^{2}k(v)dv\times f^{(2)}(x)]^{2}\}^{1/5}`</p>
 <br>
 <p>Notice that this `h_{opt}` depends on `x`, which means it is a point estimator. One disadvantange of point estimator is that 
  the optimal `h` at one point may be different from the optimal `h` chosen at another point. For this reason, we also look at the 
  integrated MSE(IMSE) of \hat{f}(x).</p>
 <br>
 <center>`[[IMSE(\hat{f}), \equiv, \int E[\hat{f}(x)-f(x)]^{2}dx],
   [,=,\frac{1}{4}h^{4}(\int v^{2}k(v)dv)^{2}\int[f^{(2)}(x)]^{2}dx+\frac{\int k^{2}(v)dv}{nh}+o(h^{4}+(nh)^{-1})]]`
</center>
 <br>
 <p>Some calculus leads to `h_{opt}=c_{0}n^{-1/5}`, where 
   `c_{0}=(\int v^{2}k(v)dv)^{-2/5}\times(\int k^{2}(v)dv)^{1/5}\times\{\int[f^{(2)}(x)]^{2}dx\}^{-1/5}`</p>
 
 <div class = 'mycode'>
 <code>
 </code>
 </div>

 
  

  
</div>
<center></center>
<center></center>
  <center></center>
<center></center><center></center>
<center></center><center></center>
<center></center><center></center>
<center></center>
  
<div class = 'references'>
  References Books:
  <br>
  [1]. Nonparametric Econometrics
  <br>
  References Sites:
  <br>

</div>

</body>


<div>
  1. Math from http://asciimath.org/
  <br>
  2. Syntax of `asciimath` from https://runarberg.github.io/ascii2mathml/ 
  <br>
  3. Syntax of `asciimath` again, from https://dlippman.imathas.com/asciimathtex/AMT.html
</div>
